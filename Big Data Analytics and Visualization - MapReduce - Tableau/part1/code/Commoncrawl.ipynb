{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference : https://www.bellingcat.com/resources/2015/08/13/using-python-to-mine-common-crawl/\n",
    "#Python2.7\n",
    "\n",
    "guncontrolDomain = 'gun-control.procon.org/*'\n",
    "#guncontrolDomain = 'freerepublic.com'\n",
    "gunsenseDomain = 'gunviolencearchive.org/*' \n",
    "nraDomain = 'nraila.org/articles/*'\n",
    "#firearmsDomain = 'usatoday.com/story/*'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import StringIO\n",
    "import gzip\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"-d\",\"--domain\",required=True,help=\"The domain to target ie. cnn.com\")\n",
    "#args = vars(ap.parse_args())\n",
    "\n",
    "#domain = guncontrolDomain\n",
    "#domain = nraDomain\n",
    "domain = gunsenseDomain\n",
    "\n",
    "# list of indices\n",
    "index_list = [\"2019-04\",\"2019-09\",\"2019-11\",\"2019-14\",\"2019-16\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_domain(domain):\n",
    "\n",
    "    record_list = []\n",
    "    \n",
    "    print \"[*] Trying target domain: %s\" % domain\n",
    "    \n",
    "    for index in index_list:\n",
    "        \n",
    "        print \"[*] Trying index %s\" % index\n",
    "        \n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
    "        cc_url += \"url=%s&output=json\" % domain\n",
    "        print(cc_url)\n",
    "        \n",
    "        response = requests.get(cc_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            records = response.content.splitlines()\n",
    "            \n",
    "            for record in records:\n",
    "                record_list.append(json.loads(record))\n",
    "            \n",
    "            print \"[*] Added %d results.\" % len(records)\n",
    "            \n",
    "    \n",
    "    print \"[*] Found a total of %d hits.\" % len(record_list)\n",
    "    \n",
    "    return record_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(record):\n",
    "\n",
    "    offset, length = int(record['offset']), int(record['length'])\n",
    "    offset_end = offset + length - 1\n",
    "\n",
    "    \n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "    \n",
    "    \n",
    "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "    \n",
    "    \n",
    "    raw_data = StringIO.StringIO(resp.content)\n",
    "    f = gzip.GzipFile(fileobj=raw_data)\n",
    "    \n",
    "    # WARC Response\n",
    "    data = f.read()\n",
    "    \n",
    "    response = \"\"\n",
    "    \n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().split('\\r\\n\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nra = []\n",
    "def getDataFromHtmlContent(html_content,keywords):\n",
    "    #print(html_content)\n",
    "    check = False\n",
    "    soup = BeautifulSoup(html_content)\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    \n",
    "    text = soup.get_text()\n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    #print(text[:10])\n",
    "    #words = text.split()\n",
    "    for word in keywords:\n",
    "        if(word in text):\n",
    "            check = True\n",
    "            break\n",
    "            \n",
    "    if(check):\n",
    "        filename='data/cc/gunsense.txt'\n",
    "        f=open(filename,'a')\n",
    "        f.write(text+\"\\n\")\n",
    "        nra.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    #lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    #chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    #text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    #print(text)\n",
    "    #filename='data/cc/'+searchKey+'/'+searchKey+str(n)+'.txt'\n",
    "    #f=open(filename,'a')\n",
    "    #f.write(soup.find_all('body'))\n",
    "    #print(soup)\n",
    "    \n",
    "    \n",
    "    #soup.prettify()\n",
    "    \n",
    "    #print(soup.find_all('p'))\n",
    "    #filename='data/cc/'+searchKey+'/'+searchKey+str(n)+'.txt'\n",
    "    #f=open(filename,'a')\n",
    "    #f.write(html_content)\n",
    "    #for j in range((len(soup.find_all('p')))-3):\n",
    "        #if (searchKey in soup.find_all('p')[j].get_text()):\n",
    "    #    f.write(soup.find_all('p')[j].get_text())\n",
    "    \n",
    "    #f.close() \n",
    "    #return html_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_list = search_domain(domain)\n",
    "link_list   = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'digest': u'3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ',\n",
       "  u'filename': u'crawl-data/CC-MAIN-2019-04/segments/1547583705737.21/crawldiagnostics/CC-MAIN-20190120102853-20190120124853-00490.warc.gz',\n",
       "  u'length': u'595',\n",
       "  u'mime': u'unk',\n",
       "  u'mime-detected': u'application/octet-stream',\n",
       "  u'offset': u'10008244',\n",
       "  u'status': u'301',\n",
       "  u'timestamp': u'20190120113222',\n",
       "  u'url': u'http://www.gunviolencearchive.org/',\n",
       "  u'urlkey': u'org,gunviolencearchive)/'}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"gun control\",\"gun voilence\",\"nra\",\"gun laws\",\"firearms\",\"second amendment\",\"gun\"]\n",
    "for record in record_list:\n",
    "    \n",
    "    html_content = download_page(record)\n",
    "    getDataFromHtmlContent(html_content,keywords)\n",
    "   \n",
    "    #soup = BeautifulSoup(html_content)\n",
    "    #for script in soup([\"script\", \"style\"]):\n",
    "    #    script.extract()\n",
    "    \n",
    "    #text = soup.get_text()\n",
    "    \n",
    "    #lines = (line.strip() for line in text.splitlines())\n",
    "    #chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    #text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    #print(text)\n",
    "        \n",
    "    #print(soup.find(\"h1\").get_text())\n",
    "    \n",
    "    #print(soup.findAll(\"div\", {\"class\": \"asset-double-wide\"})[0].prettify())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(soup.find_all('p'))\n",
    "#     print (\"[*] Retrieved %d bytes for %s\" , (len(html_content),record['url']))\n",
    "#     print(record['url'])\n",
    "    \n",
    "#     data = getDataFromHtmlContent(html_content,n,'gunman')\n",
    "#     n=n+1\n",
    "#     print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Total articles in common crawl data:', 722)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"Total articles in common crawl data:\" ,len(nra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
